{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Machine Learning Homework 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import functools\n",
    "import io\n",
    "import sys\n",
    "import numpy.lib.recfunctions as rfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to fix a bug in np.genfromtxt when Python Version (sys.version_info) is 3 or greater. \n",
    "# https://stackoverflow.com/questions/23319266/using-numpy-genfromtxt-gives-typeerror-cant-convert-bytes-object-to-str-impl\n",
    "genfromtxt_old = np.genfromtxt\n",
    "@functools.wraps(genfromtxt_old)\n",
    "def genfromtxt_py3_fixed(f, encoding=\"utf-8\", *args, **kwargs):\n",
    "  if isinstance(f, io.TextIOBase):\n",
    "    if hasattr(f, \"buffer\") and hasattr(f.buffer, \"raw\") and \\\n",
    "    isinstance(f.buffer.raw, io.FileIO):\n",
    "      # Best case: get underlying FileIO stream (binary!) and use that\n",
    "      fb = f.buffer.raw\n",
    "      # Reset cursor on the underlying object to match that on wrapper\n",
    "      fb.seek(f.tell())\n",
    "      result = genfromtxt_old(fb, *args, **kwargs)\n",
    "      # Reset cursor on wrapper to match that of the underlying object\n",
    "      f.seek(fb.tell())\n",
    "    else:\n",
    "      # Not very good but works: Put entire contents into BytesIO object,\n",
    "      # otherwise same ideas as above\n",
    "      old_cursor_pos = f.tell()\n",
    "      fb = io.BytesIO(bytes(f.read(), encoding=encoding))\n",
    "      result = genfromtxt_old(fb, *args, **kwargs)\n",
    "      f.seek(old_cursor_pos + fb.tell())\n",
    "  else:\n",
    "    result = genfromtxt_old(f, *args, **kwargs)\n",
    "  return result\n",
    "\n",
    "if sys.version_info >= (3,):\n",
    "  np.genfromtxt = genfromtxt_py3_fixed\n",
    "\n",
    "#http://esantorella.com/2016/06/16/groupby/\n",
    "#A fast GroupBy class\n",
    "class Groupby:\n",
    "    def __init__(self, keys):\n",
    "        _, self.keys_as_int = np.unique(keys, return_inverse = True)\n",
    "        self.n_keys = max(self.keys_as_int)\n",
    "        self.set_indices()\n",
    "        \n",
    "    def set_indices(self):\n",
    "        self.indices = [[] for i in range(self.n_keys+1)]\n",
    "        for i, k in enumerate(self.keys_as_int):\n",
    "            self.indices[k].append(i)\n",
    "        self.indices = [np.array(elt) for elt in self.indices]\n",
    "        \n",
    "    def apply(self, function, vector, broadcast):\n",
    "        if broadcast:\n",
    "            result = np.zeros(len(vector))\n",
    "            for idx in self.indices:\n",
    "                result[idx] = function(vector[idx])\n",
    "        else:\n",
    "            result = np.zeros(self.n_keys)\n",
    "            for k, idx in enumerate(self.indices):\n",
    "                result[self.keys_as_int[k]] = function(vector[idx])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Assignment__:  \n",
    "    A medical claim is denoted by a claim number ('Claim.Number'). Each claim consists of one or more medical lines denoted by a claim line number ('Claim.Line.Number').\n",
    "\n",
    "1. J-codes are procedure codes that start with the letter 'J'.\n",
    "\n",
    "     A. Find the number of claim lines that have J-codes.\n",
    "\n",
    "     B. How much was paid for J-codes to providers for 'in network' claims?\n",
    "\n",
    "     C. What are the top five J-codes based on the payment to providers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful guide\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/02.09-structured-data-numpy.html  \n",
    "\n",
    "Datatypes\n",
    "https://docs.scipy.org/doc/numpy-1.12.0/reference/arrays.dtypes.html  \n",
    "\n",
    "Genfromtext\n",
    "https://docs.scipy.org/doc/numpy/user/basics.io.genfromtxt.html\n",
    "Genfromtext helps bring in delimited text (like read.csv)\n",
    "\n",
    "That being said, decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed.\n",
    "\n",
    "If in doubt, try a few popular decision tree algorithms like C4.5, C5.0, CART, and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case of errors\n",
    "#sys.setrecursionlimit(10000)\n",
    "#from io import BytesIO\n",
    "#inpstream = io.open('data\\claim.sample.csv','r')\n",
    "\n",
    "#Column names courtesy Christopher Havenstein\n",
    "names = [\"V1\",\"Claim.Number\",\"Claim.Line.Number\",\"Member.ID\",\"Provider.ID\",\"Line.Of.Business.ID\",\"Revenue.Code\",\n",
    "         \"Service.Code\",\"Place.Of.Service.Code\",\"Procedure.Code\",\"Diagnosis.Code\",\"Claim.Charge.Amount\",\"Denial.Reason.Code\",\n",
    "         \"Price.Index\",\"In.Out.Of.Network\",\"Reference.Index\",\"Pricing.Index\",\"Capitation.Index\",\"Subscriber.Payment.Amount\",\n",
    "         \"Provider.Payment.Amount\",\"Group.Index\",\"Subscriber.Index\",\"Subgroup.Index\",\"Claim.Type\",\"Claim.Subscriber.Type\",\n",
    "         \"Claim.Pre.Prince.Index\",\"Claim.Current.Status\",\"Network.ID\",\"Agreement.ID\"]\n",
    "#Datatypes  courtesy Christopher Havenstein\n",
    "types = ['S8', 'f8', 'i4', 'i4', 'S14', 'S6', 'S6', 'S6', 'S4', 'S9', 'S7', 'f8',\n",
    "         'S5', 'S3', 'S3', 'S3', 'S3', 'S3', 'f8', 'f8', 'i4', 'i4', 'i4', 'S3', \n",
    "         'S3', 'S3', 'S4', 'S14', 'S14']\n",
    "#Read file into np\n",
    "CLAIMS = np.genfromtxt('data\\claim.sample.csv', dtype=types, delimiter=',', names=True, \n",
    "                       usecols=[0,1,2,3,4,5,\n",
    "                                6,7,8,9,10,11,\n",
    "                                12,13,14,15,16,\n",
    "                                17,18,19,20,21,\n",
    "                                22,23,24,25,26,\n",
    "                                27,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. Find the number of claim lines that have J-codes.\n",
      "   The Number of claim lines that have J-codes is 472559\n",
      "B. How much was paid for J-codes to providers for in network claims?\n",
      "   The amount paid to providers for in network claims is 141530658.86941\n",
      "C. What are the top five J-codes based on the payment to providers?\n",
      "The top five J-codes based on the payment to providers is: \n",
      "[(b'\" \"', 491735.2     , b'\"I\"') (b'\" \"', 243051.137625, b'\"I\"')\n",
      " (b'\" \"', 201982.66812 , b'\"I\"') ... (b'\" \"',      0.      , b'\"I\"')\n",
      " (b'\" \"',      0.      , b'\"I\"') (b'\" \"',      0.      , b'\"I\"')]\n"
     ]
    }
   ],
   "source": [
    "# A. Find the number of claim lines that have J-codes.\n",
    "jcode = 'J'\n",
    "jcode = jcode.encode()\n",
    "\n",
    "#Find the index numbers for each row that has a J and store in an array.\n",
    "#We take the non -1 values. -1 means false, we do not have a J\n",
    "jcodeIndexes = np.flatnonzero(np.core.defchararray.startswith(CLAIMS['ProcedureCode'], jcode, start=0, end=None)!=-1)\n",
    "#Create a subset of claims that have Jcodes by matching the JcodeIndexes to the indexes in CLAIMS\n",
    "jcode = CLAIMS[jcodeIndexes]\n",
    "\n",
    "derp=str(jcode.shape[0])\n",
    "print('A. Find the number of claim lines that have J-codes.')\n",
    "print('   The Number of claim lines that have J-codes is '+derp)\n",
    "\n",
    "# B. How much was paid for J-codes to providers for 'in network' claims?\n",
    "#Subset jcode to include only inNetwork claim payments\n",
    "inNetworkCode = 'I'\n",
    "inNetworkCode = inNetworkCode.encode()\n",
    "inNetworkCodeIndexes = np.flatnonzero(np.core.defchararray.find(jcode['InOutOfNetwork'],inNetworkCode)!=-1)\n",
    "inNetwork = jcode[inNetworkCodeIndexes]\n",
    "#create arrays\n",
    "ProviderPayments = inNetwork['ProviderPaymentAmount']\n",
    "jcodes = inNetwork['ProcedureCode']   \n",
    "inOutOfNetwork = inNetwork['InOutOfNetwork']\n",
    "#examine data\n",
    "# print(jcodes.dtype)\n",
    "# print(ProviderPayments.dtype)\n",
    "# print(inOutOfNetwork.dtype)\n",
    "#peek at data\n",
    "#jcodes[:3]\n",
    "#ProviderPayments[:3]\n",
    "# inOutOfNetwork[:3]\n",
    "#Join 3 arrays together\n",
    "arrays = [jcodes, ProviderPayments,inOutOfNetwork]\n",
    "#merge 3 arrays\n",
    "jcodes_with_ProviderPaymentsAndNetwork = rfn.merge_arrays(arrays, flatten = True, usemask = False)\n",
    "#examine data\n",
    "# print(jcodes_with_ProviderPaymentsAndNetwork[:3])\n",
    "#print(jcodes_with_ProviderPaymentsAndNetwork.dtype)\n",
    "# print(jcodes_with_ProviderPaymentsAndNetwork.shape)\n",
    "#print(jcodes_with_ProviderPaymentsAndNetwork.dtype.names)\n",
    "inNetwork_sum = np.sum(jcodes_with_ProviderPaymentsAndNetwork['f1'])\n",
    "print('B. How much was paid for J-codes to providers for in network claims?')\n",
    "print('   The amount paid to providers for in network claims is '+str(inNetwork_sum))\n",
    "\n",
    "# C. What are the top five J-codes based on the payment to providers?\n",
    "sortedByPayments = np.sort(jcodes_with_ProviderPaymentsAndNetwork, order='f1')\n",
    "print('C. What are the top five J-codes based on the payment to providers?')\n",
    "print('The top five J-codes based on the payment to providers are: ')\n",
    "print(sortedByPayments[::-4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For the following exercises, determine the number of providers that were paid for at least one J-code. Use the J-code claims for these providers to complete the following exercises.\n",
    "\n",
    "    A. Create a scatter plot that displays the number of unpaid claims (lines where the �Provider.Payment.Amount� field is equal to zero) for each provider versus the number of paid claims.\n",
    "\n",
    "    B. What insights can you suggest from the graph?\n",
    "\n",
    "    C. Based on the graph, is the behavior of any of the providers concerning? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#determine the number of providers that were paid for at least one J-code\n",
    "unpaid_mask = (jcode['ProviderPaymentAmount'] == 0)\n",
    "paid_mask = (jcode['ProviderPaymentAmount'] > 0)\n",
    "Unpaid_Jcodes = jcode[unpaid_mask]\n",
    "Paid_Jcodes = jcode[paid_mask]\n",
    "\n",
    "#Examine Data\n",
    "# print(Unpaid_Jcodes.dtype.names)\n",
    "# print(Unpaid_Jcodes[:10])\n",
    "# print(Unpaid_Jcodes.dtype.descr)\n",
    "\n",
    "#creating user defined datatypes\n",
    "new_dtype1 = np.dtype(Unpaid_Jcodes.dtype.descr + [('IsUnpaid', '<i4')])\n",
    "new_dtype2 = np.dtype(Paid_Jcodes.dtype.descr + [('IsUnpaid', '<i4')])\n",
    "\n",
    "#Set the shape of new matices\n",
    "Unpaid_Jcodes_w_L = np.zeros(Unpaid_Jcodes.shape, dtype=new_dtype1)\n",
    "Paid_Jcodes_w_L = np.zeros(Paid_Jcodes.shape, dtype=new_dtype2)\n",
    "\n",
    "#Examine Data\n",
    "# Unpaid_Jcodes_w_L.shape\n",
    "# print(Unpaid_Jcodes_w_L)\n",
    "\n",
    "Unpaid_Jcodes_w_L['V1'] = Unpaid_Jcodes['V1']\n",
    "Unpaid_Jcodes_w_L['ClaimNumber'] = Unpaid_Jcodes['ClaimNumber']\n",
    "Unpaid_Jcodes_w_L['ClaimLineNumber'] = Unpaid_Jcodes['ClaimLineNumber']\n",
    "Unpaid_Jcodes_w_L['MemberID'] = Unpaid_Jcodes['MemberID']\n",
    "Unpaid_Jcodes_w_L['ProviderID'] = Unpaid_Jcodes['ProviderID']\n",
    "Unpaid_Jcodes_w_L['LineOfBusinessID'] = Unpaid_Jcodes['LineOfBusinessID']\n",
    "Unpaid_Jcodes_w_L['RevenueCode'] = Unpaid_Jcodes['RevenueCode']\n",
    "Unpaid_Jcodes_w_L['ServiceCode'] = Unpaid_Jcodes['ServiceCode']\n",
    "Unpaid_Jcodes_w_L['PlaceOfServiceCode'] = Unpaid_Jcodes['PlaceOfServiceCode']\n",
    "Unpaid_Jcodes_w_L['ProcedureCode'] = Unpaid_Jcodes['ProcedureCode']\n",
    "Unpaid_Jcodes_w_L['DiagnosisCode'] = Unpaid_Jcodes['DiagnosisCode']\n",
    "Unpaid_Jcodes_w_L['ClaimChargeAmount'] = Unpaid_Jcodes['ClaimChargeAmount']\n",
    "Unpaid_Jcodes_w_L['DenialReasonCode'] = Unpaid_Jcodes['DenialReasonCode']\n",
    "Unpaid_Jcodes_w_L['PriceIndex'] = Unpaid_Jcodes['PriceIndex']\n",
    "Unpaid_Jcodes_w_L['InOutOfNetwork'] = Unpaid_Jcodes['InOutOfNetwork']\n",
    "Unpaid_Jcodes_w_L['ReferenceIndex'] = Unpaid_Jcodes['ReferenceIndex']\n",
    "Unpaid_Jcodes_w_L['PricingIndex'] = Unpaid_Jcodes['PricingIndex']\n",
    "Unpaid_Jcodes_w_L['CapitationIndex'] = Unpaid_Jcodes['CapitationIndex']\n",
    "Unpaid_Jcodes_w_L['SubscriberPaymentAmount'] = Unpaid_Jcodes['SubscriberPaymentAmount']\n",
    "Unpaid_Jcodes_w_L['ProviderPaymentAmount'] = Unpaid_Jcodes['ProviderPaymentAmount']\n",
    "Unpaid_Jcodes_w_L['GroupIndex'] = Unpaid_Jcodes['GroupIndex']\n",
    "Unpaid_Jcodes_w_L['SubscriberIndex'] = Unpaid_Jcodes['SubscriberIndex']\n",
    "Unpaid_Jcodes_w_L['SubgroupIndex'] = Unpaid_Jcodes['SubgroupIndex']\n",
    "Unpaid_Jcodes_w_L['ClaimType'] = Unpaid_Jcodes['ClaimType']\n",
    "Unpaid_Jcodes_w_L['ClaimSubscriberType'] = Unpaid_Jcodes['ClaimSubscriberType']\n",
    "Unpaid_Jcodes_w_L['ClaimPrePrinceIndex'] = Unpaid_Jcodes['ClaimPrePrinceIndex']\n",
    "Unpaid_Jcodes_w_L['ClaimCurrentStatus'] = Unpaid_Jcodes['ClaimCurrentStatus']\n",
    "Unpaid_Jcodes_w_L['NetworkID'] = Unpaid_Jcodes['NetworkID']\n",
    "Unpaid_Jcodes_w_L['AgreementID'] = Unpaid_Jcodes['AgreementID']\n",
    "#Target label unpaid = 1 (true)\n",
    "Unpaid_Jcodes_w_L['IsUnpaid'] = 1\n",
    "# Do the same for the Paid set.\n",
    "Paid_Jcodes_w_L['V1'] = Paid_Jcodes['V1']\n",
    "Paid_Jcodes_w_L['ClaimNumber'] = Paid_Jcodes['ClaimNumber']\n",
    "Paid_Jcodes_w_L['ClaimLineNumber'] = Paid_Jcodes['ClaimLineNumber']\n",
    "Paid_Jcodes_w_L['MemberID'] = Paid_Jcodes['MemberID']\n",
    "Paid_Jcodes_w_L['ProviderID'] = Paid_Jcodes['ProviderID']\n",
    "Paid_Jcodes_w_L['LineOfBusinessID'] = Paid_Jcodes['LineOfBusinessID']\n",
    "Paid_Jcodes_w_L['RevenueCode'] = Paid_Jcodes['RevenueCode']\n",
    "Paid_Jcodes_w_L['ServiceCode'] = Paid_Jcodes['ServiceCode']\n",
    "Paid_Jcodes_w_L['PlaceOfServiceCode'] = Paid_Jcodes['PlaceOfServiceCode']\n",
    "Paid_Jcodes_w_L['ProcedureCode'] = Paid_Jcodes['ProcedureCode']\n",
    "Paid_Jcodes_w_L['DiagnosisCode'] = Paid_Jcodes['DiagnosisCode']\n",
    "Paid_Jcodes_w_L['ClaimChargeAmount'] = Paid_Jcodes['ClaimChargeAmount']\n",
    "Paid_Jcodes_w_L['DenialReasonCode'] = Paid_Jcodes['DenialReasonCode']\n",
    "Paid_Jcodes_w_L['PriceIndex'] = Paid_Jcodes['PriceIndex']\n",
    "Paid_Jcodes_w_L['InOutOfNetwork'] = Paid_Jcodes['InOutOfNetwork']\n",
    "Paid_Jcodes_w_L['ReferenceIndex'] = Paid_Jcodes['ReferenceIndex']\n",
    "Paid_Jcodes_w_L['PricingIndex'] = Paid_Jcodes['PricingIndex']\n",
    "Paid_Jcodes_w_L['CapitationIndex'] = Paid_Jcodes['CapitationIndex']\n",
    "Paid_Jcodes_w_L['SubscriberPaymentAmount'] = Paid_Jcodes['SubscriberPaymentAmount']\n",
    "Paid_Jcodes_w_L['ProviderPaymentAmount'] = Paid_Jcodes['ProviderPaymentAmount']\n",
    "Paid_Jcodes_w_L['GroupIndex'] = Paid_Jcodes['GroupIndex']\n",
    "Paid_Jcodes_w_L['SubscriberIndex'] = Paid_Jcodes['SubscriberIndex']\n",
    "Paid_Jcodes_w_L['SubgroupIndex'] = Paid_Jcodes['SubgroupIndex']\n",
    "Paid_Jcodes_w_L['ClaimType'] = Paid_Jcodes['ClaimType']\n",
    "Paid_Jcodes_w_L['ClaimSubscriberType'] = Paid_Jcodes['ClaimSubscriberType']\n",
    "Paid_Jcodes_w_L['ClaimPrePrinceIndex'] = Paid_Jcodes['ClaimPrePrinceIndex']\n",
    "Paid_Jcodes_w_L['ClaimCurrentStatus'] = Paid_Jcodes['ClaimCurrentStatus']\n",
    "Paid_Jcodes_w_L['NetworkID'] = Paid_Jcodes['NetworkID']\n",
    "Paid_Jcodes_w_L['AgreementID'] = Paid_Jcodes['AgreementID']\n",
    "#Target label unpaid = 0 (false)\n",
    "Paid_Jcodes_w_L['IsUnpaid'] = 0\n",
    "#now combine the rows together (axis=0)\n",
    "Jcodes_w_L = np.concatenate((Unpaid_Jcodes_w_L, Paid_Jcodes_w_L), axis=0)\n",
    "#look at the transition between the rows around row 44961. Need to shuffle\n",
    "np.random.shuffle(Jcodes_w_L)\n",
    "# print(Jcodes_w_L[:100])\n",
    "# Jcodes_w_L.shape\n",
    "\n",
    "# #format for sklearn\n",
    "label =  'IsUnpaid'\n",
    "cat_features = ['V1', 'ProviderID','LineOfBusinessID','RevenueCode',\n",
    "                'ServiceCode', 'PlaceOfServiceCode', 'ProcedureCode',\n",
    "                'DiagnosisCode', 'DenialReasonCode',\n",
    "                'PriceIndex', 'InOutOfNetwork', 'ReferenceIndex', \n",
    "                'PricingIndex', 'CapitationIndex', 'ClaimSubscriberType',\n",
    "                'ClaimPrePrinceIndex', 'ClaimCurrentStatus', 'NetworkID',\n",
    "                'AgreementID', 'ClaimType', ]\n",
    "numeric_features = ['ClaimNumber', 'ClaimLineNumber', 'MemberID', \n",
    "                    'ClaimChargeAmount',\n",
    "                    'SubscriberPaymentAmount', 'ProviderPaymentAmount',\n",
    "                    'GroupIndex', 'SubscriberIndex', 'SubgroupIndex']\n",
    "\n",
    "#separate categorical and numeric features. \n",
    "#We put them into list and back into nparray again to reformat so Scikitlearn can see the columns\n",
    "Mcat = np.array(Jcodes_w_L[cat_features].tolist())\n",
    "Mnum = np.array(Jcodes_w_L[numeric_features].tolist())\n",
    "L = np.array(Jcodes_w_L[label].tolist())\n",
    "\n",
    "# L.shape\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABELS = ['ProviderID1','ProviderIDN'] #etc\n",
    "\n",
    "\n",
    "# #Produce the scatterplot as the answer to 2a\n",
    "# FIG, AX = plt.subplots()\n",
    "# AX.scatter(UNPAIDAGG, PAIDAGG)\n",
    "# AX.grid(linestyle='-', linewidth='0.75', color='red')\n",
    "\n",
    "# FIG = plt.gcf()\n",
    "# FIG.set_size_inches(25, 25)\n",
    "# plt.rcParams.update({'font.size': 28})\n",
    "\n",
    "# for i, TXT in enumerate(LABELS):\n",
    "#     AX.annotate(TXT, (UNPAIDAGG[I], PAIDAGG[I]))\n",
    "\n",
    "# plt.tick_params(labelsize=35)\n",
    "# plt.xlabel('# of Unpaid claims', fontsize=35)\n",
    "\n",
    "# plt.ylabel('# of Paid claims', fontsize=35)\n",
    "\n",
    "# plt.title('Scatterplot of Unpaid and Paid claims by Provider', fontsize=45)\n",
    "# plt.savefig('Paid_Unpaid_Scatterplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load into new arrays\n",
    "\n",
    "# #format for sklearn\n",
    "# label =  'IsUnpaid'\n",
    "# cat_features = ['V1', 'ProviderID','LineOfBusinessID','RevenueCode',\n",
    "#                 'ServiceCode', 'PlaceOfServiceCode', 'ProcedureCode',\n",
    "#                 'DiagnosisCode', 'DenialReasonCode',\n",
    "#                 'PriceIndex', 'InOutOfNetwork', 'ReferenceIndex', \n",
    "#                 'PricingIndex', 'CapitationIndex', 'ClaimSubscriberType',\n",
    "#                 'ClaimPrePrinceIndex', 'ClaimCurrentStatus', 'NetworkID',\n",
    "#                 'AgreementID', 'ClaimType', ]\n",
    "# numeric_features = ['ClaimNumber', 'ClaimLineNumber', 'MemberID', \n",
    "#                     'ClaimChargeAmount',\n",
    "#                     'SubscriberPaymentAmount', 'ProviderPaymentAmount',\n",
    "#                     'GroupIndex', 'SubscriberIndex', 'SubgroupIndex']\n",
    "\n",
    "# #convert features to list, then to np.array \n",
    "# # This step is important for sklearn to use the data from the structured NumPy array\n",
    "\n",
    "# #separate categorical and numeric features. \n",
    "# #We put them into list and back into nparray again to reformat so Scikitlearn can see the columns\n",
    "# Mcat = np.array(Jcodes_w_L[cat_features].tolist())\n",
    "# Mnum = np.array(Jcodes_w_L[numeric_features].tolist())\n",
    "# L = np.array(Jcodes_w_L[label].tolist())\n",
    "\n",
    "# # first use Sklearn's LabelEncoder function ... then use the OneHotEncoder function\n",
    "# # https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621\n",
    "# # http://www.stephacking.com/encode-categorical-data-labelencoder-onehotencoder-python/\n",
    "\n",
    "# # Some claim you can do OnehotEncoder without a label encoder, but I haven't seen it work.\n",
    "# # https://stackoverflow.com/questions/48929124/scikit-learn-how-to-compose-labelencoder-and-onehotencoder-with-a-pipeline\n",
    "\n",
    "# # Run the Label encoder\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# for i in range(20):\n",
    "#    Mcat[:,i] = le.fit_transform(Mcat[:,i])\n",
    "\n",
    "# # Run the OneHotEncoder\n",
    "# # Could encounter a memory error here in which case, you probably should subset.\n",
    "# ohe = OneHotEncoder(sparse=False) #Easier to read\n",
    "# Mcat = ohe.fit_transform(Mcat)\n",
    "\n",
    "# #What is the shape of the matrix categorical columns that were OneHotEncoded?   \n",
    "# Mcat.shape\n",
    "# Mnum.shape\n",
    "\n",
    "\n",
    "# #I am subsetting them since I was having memory issues.\n",
    "# #You might be able to decide which features are useful and remove some of them before\n",
    "# # the label encoder and one hot encoding step\n",
    "\n",
    "# #If you want to recover from the memory error then subset\n",
    "# #Mcat = np.array(Jcodes_w_L[cat_features].tolist())\n",
    "\n",
    "# Mcat_subset = Mcat[0:5000]\n",
    "# Mcat=Mcat_subset\n",
    "\n",
    "# Mnum_subset = Mnum[0:5000]\n",
    "# Mnum=Mnum_subset\n",
    "\n",
    "# L_subset = L[0:5000]\n",
    "# L=L_subset\n",
    "\n",
    "# # Uncomment if you need to run again from a subset.\n",
    "\n",
    "# ## Run the Label encoder\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# for i in range(20):\n",
    "#    Mcat[:,i] = le.fit_transform(Mcat[:,i])\n",
    "\n",
    "# # Run the OneHotEncoder\n",
    "# # Could encounter a memory error here in which case, you probably should subset.\n",
    "# ohe = OneHotEncoder(sparse=False) #Easier to read\n",
    "# Mcat = ohe.fit_transform(Mcat)\n",
    "\n",
    "\n",
    "# #What is the size in megabytes before subsetting?\n",
    "# # https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-33.php\n",
    "# # and using base2 (binary conversion), https://www.gbmb.org/bytes-to-mb\n",
    "# print(\"%d Megabytes\" % ((Mcat.size * Mcat.itemsize)/1048576))\n",
    "# print(\"%d Megabytes\" % ((Mnum.size * Mnum.itemsize)/1048576))\n",
    "\n",
    "# #What is the size in megabytes after subsetting?\n",
    "# print(\"%d Megabytes\" % ((Mcat_subset.size * Mcat_subset.itemsize)/1048576)) \n",
    "# print(\"%d Megabytes\" % ((Mnum_subset.size * Mnum_subset.itemsize)/1048576))\n",
    "\n",
    "\n",
    "# M = np.concatenate((Mcat, Mnum), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# #Concatenate the columns\n",
    "# M = np.concatenate((Mcat_subset, Mnum_subset), axis=1)\n",
    "\n",
    "\n",
    "# L = Jcodes_w_L[label].astype(int)\n",
    "\n",
    "# # Match the label rows to the subset matrix rows.\n",
    "# L = L[0:5000]\n",
    "\n",
    "# M.shape\n",
    "# L.shape\n",
    "\n",
    "# # Now you can use your DeathToGridsearch code.\n",
    "\n",
    "\n",
    "# n_folds = 5\n",
    "\n",
    "# #EDIT: pack the arrays together into \"data\"\n",
    "# data = (M,L,n_folds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #EDIT: A function, \"run\", to run all our classifiers against our data.\n",
    "\n",
    "# def run(a_clf, data, clf_hyper={}):\n",
    "#   M, L, n_folds = data #EDIT: unpack the \"data\" container of arrays\n",
    "#   kf = KFold(n_splits=n_folds) # JS: Establish the cross validation \n",
    "#   ret = {} # JS: classic explicaiton of results\n",
    "  \n",
    "#   for ids, (train_index, test_index) in enumerate(kf.split(M, L)): #EDIT: We're interating through train and test indexes by using kf.split\n",
    "#                                                                    #      from M and L.\n",
    "#                                                                    #      We're simply splitting rows into train and test rows\n",
    "#                                                                    #      for our five folds.\n",
    "    \n",
    "#     clf = a_clf(**clf_hyper) # JS: unpack paramters into clf if they exist   #EDIT: this gives all keyword arguments except \n",
    "#                                                                              #      for those corresponding to a formal parameter\n",
    "#                                                                              #      in a dictionary.\n",
    "            \n",
    "#     clf.fit(M[train_index], L[train_index])   #EDIT: First param, M when subset by \"train_index\", \n",
    "#                                               #      includes training X's. \n",
    "#                                               #      Second param, L when subset by \"train_index\",\n",
    "#                                               #      includes training Y.                             \n",
    "    \n",
    "#     pred = clf.predict(M[test_index])         #EDIT: Using M -our X's- subset by the test_indexes, \n",
    "#                                               #      predict the Y's for the test rows.\n",
    "    \n",
    "#     ret[ids]= {'clf': clf,                    #EDIT: Create arrays of\n",
    "#                'train_index': train_index,\n",
    "#                'test_index': test_index,\n",
    "#                'accuracy': accuracy_score(L[test_index], pred)}    \n",
    "#   return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def populateClfAccuracyDict(results):\n",
    "#     for key in results:\n",
    "#         k1 = results[key]['clf'] \n",
    "#         v1 = results[key]['accuracy']\n",
    "#         k1Test = str(k1) #Since we have a number of k-folds for each classifier...\n",
    "#                          #We want to prevent unique k1 values due to different \"key\" values\n",
    "#                          #when we actually have the same classifer and hyper parameter settings.\n",
    "#                          #So, we convert to a string\n",
    "                        \n",
    "#         #String formatting            \n",
    "#         k1Test = k1Test.replace('            ',' ') # remove large spaces from string\n",
    "#         k1Test = k1Test.replace('          ',' ')\n",
    "        \n",
    "#         #Then check if the string value 'k1Test' exists as a key in the dictionary\n",
    "#         if k1Test in clfsAccuracyDict:\n",
    "#             clfsAccuracyDict[k1Test].append(v1) #append the values to create an array (techically a list) of values\n",
    "#         else:\n",
    "#             clfsAccuracyDict[k1Test] = [v1] #create a new key (k1Test) in clfsAccuracyDict with a new value, (v1)            \n",
    "        \n",
    "            \n",
    "\n",
    "# def myHyperSetSearch(clfsList,clfDict):\n",
    "#     #hyperSet = {}\n",
    "#     for clf in clfsList:\n",
    "    \n",
    "#     #I need to check if values in clfsList are in clfDict\n",
    "#         clfString = str(clf)\n",
    "#         #print(\"clf: \", clfString)\n",
    "        \n",
    "#         for k1, v1 in clfDict.items(): # go through the inner dictionary of hyper parameters\n",
    "#             #Nothing to do here, we need to get into the inner nested dictionary.\n",
    "#             if k1 in clfString:\n",
    "#                 #allows you to do all the matching key and values\n",
    "#                 k2,v2 = zip(*v1.items()) # explain zip (https://docs.python.org/3.3/library/functions.html#zip)\n",
    "#                 for values in product(*v2): #for the values in the inner dictionary, get their unique combinations from product()\n",
    "#                     hyperSet = dict(zip(k2, values)) # create a dictionary from their values\n",
    "#                     results = run(clf, data, hyperSet) # pass the clf and dictionary of hyper param combinations to run; get results\n",
    "#                     populateClfAccuracyDict(results) # populate clfsAccuracyDict with results\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# clfsList = [RandomForestClassifier] \n",
    "\n",
    "# clfDict = {'RandomForestClassifier': {\"min_samples_split\": [2,3,4], \n",
    "#                                       \"n_jobs\": [1,2,3]}}#,\n",
    "                                      \n",
    "#            #'LogisticRegression': {\"tol\": [0.001,0.01,0.1]}}\n",
    "\n",
    "                   \n",
    "# #Declare empty clfs Accuracy Dict to populate in myHyperSetSearch     \n",
    "# clfsAccuracyDict = {}\n",
    "\n",
    "# #Run myHyperSetSearch\n",
    "# myHyperSetSearch(clfsList,clfDict)    \n",
    "\n",
    "# print(clfsAccuracyDict)\n",
    "\n",
    "\n",
    "# # for determining maximum frequency (# of kfolds) for histogram y-axis\n",
    "# n = max(len(v1) for k1, v1 in clfsAccuracyDict.items())\n",
    "\n",
    "# # for naming the plots\n",
    "# filename_prefix = 'clf_Histograms_'\n",
    "\n",
    "# # initialize the plot_num counter for incrementing in the loop below\n",
    "# plot_num = 1 \n",
    "\n",
    "# # Adjust matplotlib subplots for easy terminal window viewing\n",
    "# left  = 0.125  # the left side of the subplots of the figure\n",
    "# right = 0.9    # the right side of the subplots of the figure\n",
    "# bottom = 0.1   # the bottom of the subplots of the figure\n",
    "# top = 0.6      # the top of the subplots of the figure\n",
    "# wspace = 0.2   # the amount of width reserved for space between subplots,\n",
    "#                # expressed as a fraction of the average axis width\n",
    "# hspace = 0.2   # the amount of height reserved for space between subplots,\n",
    "#                # expressed as a fraction of the average axis height\n",
    "               \n",
    "\n",
    "\n",
    "# #create the histograms\n",
    "# for k1, v1 in clfsAccuracyDict.items():\n",
    "#     # for each key in our clfsAccuracyDict, create a new histogram with a given key's values \n",
    "#     fig = plt.figure(figsize =(20,10)) # This dictates the size of our histograms\n",
    "#     ax  = fig.add_subplot(1, 1, 1) # As the ax subplot numbers increase here, the plot gets smaller\n",
    "#     plt.hist(v1, facecolor='green', alpha=0.75) # create the histogram with the values\n",
    "#     ax.set_title(k1, fontsize=30) # increase title fontsize for readability\n",
    "#     ax.set_xlabel('Classifer Accuracy (By K-Fold)', fontsize=25) # increase x-axis label fontsize for readability\n",
    "#     ax.set_ylabel('Frequency', fontsize=25) # increase y-axis label fontsize for readability\n",
    "#     ax.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) # The accuracy can only be from 0 to 1 (e.g. 0 or 100%)\n",
    "#     ax.yaxis.set_ticks(np.arange(0, n+1, 1)) # n represents the number of k-folds\n",
    "#     ax.xaxis.set_tick_params(labelsize=20) # increase x-axis tick fontsize for readability\n",
    "#     ax.yaxis.set_tick_params(labelsize=20) # increase y-axis tick fontsize for readability\n",
    "#     #ax.grid(True) # you can turn this on for a grid, but I think it looks messy here.\n",
    "\n",
    "#     # pass in subplot adjustments from above.\n",
    "#     plt.subplots_adjust(left=left, right=right, bottom=bottom, top=top, wspace=wspace, hspace=hspace)\n",
    "#     plot_num_str = str(plot_num) #convert plot number to string\n",
    "#     filename = filename_prefix + plot_num_str # concatenate the filename prefix and the plot_num_str\n",
    "#     plt.savefig(filename, bbox_inches = 'tight') # save the plot to the user's working directory\n",
    "#     plot_num = plot_num+1 # increment the plot_num counter by 1\n",
    "    \n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Consider all claim lines with a J-code.\n",
    "\n",
    "     A. What percentage of J-code claim lines were unpaid?\n",
    "\n",
    "     B. Create a model to predict when a J-code is unpaid. Explain why you choose the modeling approach.\n",
    "\n",
    "     C. How accurate is your model at predicting unpaid claims?\n",
    "\n",
    "      D. What data attributes are predominately influencing the rate of non-payment?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
