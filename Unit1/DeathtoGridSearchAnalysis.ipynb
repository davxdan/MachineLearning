{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as numpy\n",
    "from sklearn.metrics import accuracy_score # other metrics?\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt this code below to run your analysis\n",
    "\n",
    "Due before live class 2\n",
    "1. Write a function to take a list or dictionary of clfs and hypers ie use logistic regression, each with 3 different sets of hyper parrameters for each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Wikipedia, the free encyclopedia  \n",
    "\n",
    "In machine learning, __hyperparameter__ optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.\n",
    "\n",
    "The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data.[1] The objective function takes a tuple of hyperparameters and returns the associated loss.[1] Cross-validation is often used to estimate this generalization performance.[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M = numpy.array([[1,2],[3,4],[4,5],[4,5],[4,5],[4,5],[4,5],[4,5]])\n",
    "L = numpy.random.choice([0,1],size=(M.shape[0])) #creates an array of 1's in the same as rows in M\n",
    "n_folds = 5\n",
    "\n",
    "data = (M, L, n_folds) #creates a tuple; a tuple is same as list but immutable (Cant be changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Classifier Algorithm Lists and Dictionaries__  \n",
    "I need to revisit to make this take in data like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifierDecriptionString:  <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "min_samples_split\n",
      "2\n",
      "3\n",
      "4\n",
      "classifierDecriptionString:  <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "tol\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "None\n",
      "classifierDecriptionString:  <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "min_samples_split\n",
      "2\n",
      "3\n",
      "4\n",
      "classifierDecriptionString:  <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "tol\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#copied and adapted from Christopher Havenstein office hours presentation\n",
    "\n",
    "classifierList = [RandomForestClassifier, LogisticRegression]\n",
    "classifierParametersDictionary = {'RandomForestClassifier': {\"min_samples_split\": [2,3,4]},\n",
    "           'LogisticRegression':{\"tol\":[0.001,0.01,0.1]}}\n",
    "\n",
    "def classifierFunction(classifierList):\n",
    "    for description in (classifierList):\n",
    "        #check if values in clfslist are in clfDict\n",
    "        classifierDecriptionString = str(description)\n",
    "        print(\"classifierDecriptionString: \", classifierDecriptionString)\n",
    "        \n",
    "        for outerKeys, outerValues in classifierParametersDictionary.items(): #go through first level of clfDict\n",
    "            if outerKeys in classifierDecriptionString: #if clfString matches\n",
    "                for innerKeys,innerValues in outerValues.items(): #go through inner dictionary of hypers\n",
    "                    print(innerKeys) #for each yper parameter in the inner list \n",
    "                    for values in innerValues: # go through the values for each hyper parameter\n",
    "                        print(values) # and show them\n",
    "for clfs in classifierList:\n",
    "    results = classifierFunction(classifierList)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k fold =  0\n",
      "            train indexes =  [2 3 4 5 6 7]\n",
      "            test indexes =  [0 1]\n",
      "k fold =  1\n",
      "            train indexes =  [0 1 4 5 6 7]\n",
      "            test indexes =  [2 3]\n",
      "k fold =  2\n",
      "            train indexes =  [0 1 2 3 6 7]\n",
      "            test indexes =  [4 5]\n",
      "k fold =  3\n",
      "            train indexes =  [0 1 2 3 4 5 7]\n",
      "            test indexes =  [6]\n",
      "k fold =  4\n",
      "            train indexes =  [0 1 2 3 4 5 6]\n",
      "            test indexes =  [7]\n",
      "{0: ({'Classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'train_index': array([2, 3, 4, 5, 6, 7]), 'test_index': array([0, 1]), 'accuracy': 0.0},), 1: ({'Classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'train_index': array([0, 1, 4, 5, 6, 7]), 'test_index': array([2, 3]), 'accuracy': 0.5},), 2: ({'Classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'train_index': array([0, 1, 2, 3, 6, 7]), 'test_index': array([4, 5]), 'accuracy': 0.5},), 3: ({'Classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'train_index': array([0, 1, 2, 3, 4, 5, 7]), 'test_index': array([6]), 'accuracy': 0.0},), 4: ({'Classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'train_index': array([0, 1, 2, 3, 4, 5, 6]), 'test_index': array([7]), 'accuracy': 0.0},)}\n",
      "k fold =  0\n",
      "            train indexes =  [2 3 4 5 6 7]\n",
      "            test indexes =  [0 1]\n",
      "k fold =  1\n",
      "            train indexes =  [0 1 4 5 6 7]\n",
      "            test indexes =  [2 3]\n",
      "k fold =  2\n",
      "            train indexes =  [0 1 2 3 6 7]\n",
      "            test indexes =  [4 5]\n",
      "k fold =  3\n",
      "            train indexes =  [0 1 2 3 4 5 7]\n",
      "            test indexes =  [6]\n",
      "k fold =  4\n",
      "            train indexes =  [0 1 2 3 4 5 6]\n",
      "            test indexes =  [7]\n",
      "{0: ({'Classifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=2,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), 'train_index': array([2, 3, 4, 5, 6, 7]), 'test_index': array([0, 1]), 'accuracy': 1.0},), 1: ({'Classifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=2,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), 'train_index': array([0, 1, 4, 5, 6, 7]), 'test_index': array([2, 3]), 'accuracy': 0.5},), 2: ({'Classifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=2,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), 'train_index': array([0, 1, 2, 3, 6, 7]), 'test_index': array([4, 5]), 'accuracy': 0.5},), 3: ({'Classifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=2,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), 'train_index': array([0, 1, 2, 3, 4, 5, 7]), 'test_index': array([6]), 'accuracy': 0.0},), 4: ({'Classifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=2,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), 'train_index': array([0, 1, 2, 3, 4, 5, 6]), 'test_index': array([7]), 'accuracy': 1.0},)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1232: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    }
   ],
   "source": [
    "def run(a_Classifier, data, Classifier_hyper={}): # {} are dictionary and [] are for list\n",
    "  M, L, n_folds = data # unpack data containter\n",
    "  kf = KFold(n_splits=n_folds) # Establish the cross validation\n",
    "  Classifier_hyper= {'n_jobs': 2}\n",
    "  ret = {} # classic explicaiton of results as dictionary\n",
    "  for ids, (train_index, test_index) in enumerate(kf.split(M, L)):\n",
    "        print(\"k fold = \", ids)\n",
    "        print(\"            train indexes = \", train_index)\n",
    "        print(\"            test indexes = \", test_index)\n",
    "        Classifier = a_Classifier(**Classifier_hyper) # unpack paramters into clf is they exist\n",
    "        Classifier.fit(M[train_index], L[train_index])\n",
    "        pred = Classifier.predict(M[test_index])\n",
    "        ret[ids]= {'Classifier': Classifier,\n",
    "               'train_index': train_index,\n",
    "               'test_index': test_index,\n",
    "               'accuracy': accuracy_score(L[test_index], pred)},\n",
    "  return ret\n",
    "\n",
    "algorithmlist = [RandomForestClassifier,LogisticRegression]\n",
    "for algorithms in algorithmlist:\n",
    "    results = run(algorithms, data, Classifier_hyper={})\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due before live class 3\n",
    "2. expand to include larger number of classifiers and hyperparmater settings\n",
    "3. find some simple data\n",
    "4. generate matplotlib plots that will assist in identifying the optimal clf and parampters settings\n",
    "\n",
    "Due before live class 4\n",
    "5. Please set up your code to be run and save the results to the directory that its executed from\n",
    "6. Investigate grid search function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
